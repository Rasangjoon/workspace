{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작업이 완료되었습니다. CSV 파일을 확인해보세요.\n"
     ]
    }
   ],
   "source": [
    "#전체 항목 크롤링.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "# 키워드와 페이지 수 입력 받기\n",
    "keyword = input('키워드를 입력하세요:')\n",
    "allPage = input('몇 페이지까지 추출하시겠어요?')\n",
    "\n",
    "# CSV 파일 열기\n",
    "with open('dataana7.csv', 'w', newline='', encoding='CP949') as csvfile:\n",
    "    # CSV 파일에 데이터를 작성하기 위한 writer 객체 생성\n",
    "    fieldnames = ['Date', 'Title', 'Company', 'URL', 'Deadline', 'Location', 'Experience', 'Requirement', 'Job Type']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "    # CSV 파일에 헤더(열 이름)를 쓰기\n",
    "    writer.writeheader()\n",
    "\n",
    "    for page in range(1, int(allPage)+1): \n",
    "        soup = requests.get('https://www.saramin.co.kr/zf_user/search/recruit?search_area=main&search_done=y&search_optional_item=n&searchType=search&searchword={}&recruitPage={}&recruitSort=relation&recruitPageCount=100'.format(keyword, page), headers={'User-Agent':'Mozilla/5.0'})\n",
    "        html = BeautifulSoup(soup.text, 'html.parser')\n",
    "        jobs = html.select('div.item_recruit')\n",
    "        \n",
    "        for job in jobs:\n",
    "            try:\n",
    "                today = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "                title = job.select_one('a')['title'].strip().replace(',', '')\n",
    "                company = job.select_one('div.area_corp > strong > a').text.strip()\n",
    "                url = 'http://www.saramin.co.kr' + job.select_one('a')['href']\n",
    "                deadline = job.select_one('span.date').text.strip()\n",
    "                location = job.select('div.job_condition > span')[0].text.strip()\n",
    "                experience = job.select('div.job_condition > span')[1].text.strip()\n",
    "                requirement = job.select('div.job_condition > span')[2].text.strip()\n",
    "                jobtype = job.select('div.job_condition > span')[3].text.strip()\n",
    "                             \n",
    "                # CSV 파일에 데이터를 쓰기\n",
    "                writer.writerow({'Date': today, 'Title': title, 'Company': company, 'URL': url, 'Deadline': deadline, 'Location': location, 'Experience': experience, 'Requirement': requirement, 'Job Type': jobtype})\n",
    "                \n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "print(\"작업이 완료되었습니다. CSV 파일을 확인해보세요.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작업이 완료되었습니다. CSV 파일을 확인해보세요.\n"
     ]
    }
   ],
   "source": [
    "#신입,경력 구분해서 크롤링\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "# 키워드와 페이지 수 입력 받기\n",
    "keyword = input('키워드를 입력하세요:')\n",
    "allPage = input('몇 페이지까지 추출하시겠어요?')\n",
    "\n",
    "# CSV 파일 열기\n",
    "with open('dataana5.csv', 'w', newline='', encoding='CP949') as csvfile:\n",
    "    # CSV 파일에 데이터를 작성하기 위한 writer 객체 생성\n",
    "    fieldnames = ['Date', 'Title', 'Company', 'URL', 'Deadline', 'Location', 'Experience', 'Requirement', 'Job Type']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "    # CSV 파일에 헤더(열 이름)를 쓰기\n",
    "    writer.writeheader()\n",
    "\n",
    "    for page in range(1, int(allPage)+1): \n",
    "        soup = requests.get('https://www.saramin.co.kr/zf_user/search/recruit?search_area=main&search_done=y&search_optional_item=n&searchType=search&searchword={}&recruitPage={}&recruitSort=relation&recruitPageCount=100'.format(keyword, page), headers={'User-Agent':'Mozilla/5.0'})\n",
    "        html = BeautifulSoup(soup.text, 'html.parser')\n",
    "        jobs = html.select('div.item_recruit')\n",
    "        \n",
    "        for job in jobs:\n",
    "            try:\n",
    "                today = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "                title = job.select_one('a')['title'].strip().replace(',', '')\n",
    "                company = job.select_one('div.area_corp > strong > a').text.strip()\n",
    "                url = 'http://www.saramin.co.kr' + job.select_one('a')['href']\n",
    "                deadline = job.select_one('span.date').text.strip()\n",
    "                location = job.select('div.job_condition > span')[0].text.strip()\n",
    "                experience = job.select('div.job_condition > span')[1].text.strip()\n",
    "                #해당하는 부분에 맞게 '경력','경력무관','신입·경력'만 뽑을 수 있도록\n",
    "                if experience not in ['경력', '경력무관', '신입·경력']:\n",
    "                    continue\n",
    "                    \n",
    "                    \n",
    "                requirement = job.select('div.job_condition > span')[2].text.strip()\n",
    "                jobtype = job.select('div.job_condition > span')[3].text.strip()\n",
    "                             \n",
    "                # CSV 파일에 데이터를 쓰기\n",
    "                writer.writerow({'Date': today, 'Title': title, 'Company': company, 'URL': url, 'Deadline': deadline, 'Location': location, 'Experience': experience, 'Requirement': requirement, 'Job Type': jobtype})\n",
    "                \n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "print(\"작업이 완료되었습니다. CSV 파일을 확인해보세요.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작업이 완료되었습니다. CSV 파일을 확인해보세요.\n"
     ]
    }
   ],
   "source": [
    "#keywords 여러개 선택.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "# 키워드 입력 받기\n",
    "keywords = input('키워드를 입력하세요(여러 개의 키워드를 입력하고 각 키워드를 쉼표로 구분하세요): ').split(',')\n",
    "\n",
    "# 페이지 수 입력 받기\n",
    "allPage = input('몇 페이지까지 추출하시겠어요? ')\n",
    "\n",
    "# CSV 파일 열기\n",
    "with open('dataana10.csv', 'w', newline='', encoding='CP949') as csvfile:\n",
    "    # CSV 파일에 데이터를 작성하기 위한 writer 객체 생성\n",
    "    fieldnames = ['Date', 'Title', 'Company', 'URL', 'Deadline', 'Location', 'Experience', 'Requirement', 'Job Type']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "    # CSV 파일에 헤더(열 이름)를 쓰기\n",
    "    writer.writeheader()\n",
    "\n",
    "    for keyword in keywords:\n",
    "        for page in range(1, int(allPage) + 1): \n",
    "            soup = requests.get('https://www.saramin.co.kr/zf_user/search/recruit?search_area=main&search_done=y&search_optional_item=n&searchType=search&searchword={}&recruitPage={}&recruitSort=relation&recruitPageCount=100'.format(keyword.strip(), page), headers={'User-Agent':'Mozilla/5.0'})\n",
    "            html = BeautifulSoup(soup.text, 'html.parser')\n",
    "            jobs = html.select('div.item_recruit')\n",
    "            \n",
    "            for job in jobs:\n",
    "                try:\n",
    "                    today = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "                    title = job.select_one('a')['title'].strip().replace(',', '')\n",
    "                    company = job.select_one('div.area_corp > strong > a').text.strip()\n",
    "                    url = 'http://www.saramin.co.kr' + job.select_one('a')['href']\n",
    "                    deadline = job.select_one('span.date').text.strip()\n",
    "                    location = job.select('div.job_condition > span')[0].text.strip()\n",
    "                    experience = job.select('div.job_condition > span')[1].text.strip()\n",
    "                                    #해당하는 부분에 맞게 '경력','경력무관','신입·경력'만 뽑을 수 있도록\n",
    "                    if experience not in ['경력', '경력무관', '신입·경력']:\n",
    "                        continue\n",
    "                    \n",
    "                    requirement = job.select('div.job_condition > span')[2].text.strip()\n",
    "                    jobtype = job.select('div.job_condition > span')[3].text.strip()\n",
    "                    \n",
    "                    # CSV 파일에 데이터를 쓰기\n",
    "                    writer.writerow({'Date': today, 'Title': title, 'Company': company, 'URL': url, 'Deadline': deadline, 'Location': location, 'Experience': experience, 'Requirement': requirement, 'Job Type': jobtype})\n",
    "                    \n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "print(\"작업이 완료되었습니다. CSV 파일을 확인해보세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
